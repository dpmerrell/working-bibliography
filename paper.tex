\documentclass[14pt]{article}

%\usepackage{showframe}
\usepackage{ragged2e}
\usepackage{hyperref}
\usepackage{amsmath}
%\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{color}
\usepackage[round]{natbib}


\date{}
\setlength{\topmargin}{0pt}
\setlength{\headheight}{0pt}
\setlength{\headsep}{0pt}
\setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt}
\setlength{\textheight}{9in}
\setlength{\textwidth}{6.50in}
\setlength{\hoffset}{0.0in}
\setlength{\voffset}{-0.0in}
\renewcommand*\rmdefault{ppl}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{14pt plus4pt minus4pt}
%\renewcommand{\baselinestretch}{1.25}


\begin{document}

\title{Working Bibliography}

\author{David Merrell \\ Department of Computer Sciences \\ University of Wisconsin -- Madison}

\maketitle{}

\thispagestyle{empty}

\section{Probabilistic Programming}

\subsection{Probabilistic Metaprogramming}

\paragraph{\citet{lake-bpl-2015}. Probabilistic Program Induction.}
The authors use probabilistic programs to model hand-written characters.
They use hierarchical priors over those programs, in order to infer programs that best explain a given character.
This system is used to generate new samples that imitate the given sample, in a one-shot fashion.
That is, the system successfully learns the generative concept for one example character, and then employs that concept to produce new samples and perform classification tasks.
The system classifies characters at human levels of accuracy, and produces samples that are indistinguishable from those written by humans (this was demonstrated in via Amazon Mechanical Turk). 

\paragraph{\citet{saad-synthesis-2019}. Bayesian Synthesis of Probabilistic Programs.}
The authors describe a Bayesian framework for synthesizing probabilistic programs for data modeling tasks.
They use probabilistic context-free grammars to define prior distributions over domain-specific probabilistic programming languages.
They define domain-specific code transformations that enable MCMC sampling from the posterior distribution of programs.
They implement their framework using the Venture PPL, and demonstrate it on two tasks: (1) Gaussian process time-series regression and (2) nonparametric mixture modeling. 

\subsection{The \texttt{Gen} language}

\paragraph{\citet{cusumano-gen-2018}. Gen: A General-Purpose PPL with Programmable Inference.}
\texttt{Gen} is a probabilistic programming language implemented in Julia.
It differs from other PPLs in that it does not attempt to hide inference from the user---instead,
\texttt{Gen} attempts to make it easier for users to compose their own inference methods.
It does so by providing a concise interface for operating on program traces, along with a library of standard inference methods.
The authors compare \texttt{Gen}'s performance against other PPLs on a variety of tasks, including (a) robust Bayesian regression, (b) Gaussian process structure learning, (c) estimating 3D body poses from depth images, and (d) nonlinear state-space modeling.
Most of the well-known languages (e.g., Stan, Edward, Pyro) were not even capable of attempting all these tasks, due to their limited expressiveness (e.g., Stan's being tightly bound to hierarchical models and Pyro's focus on deep generative modeling). 

\section{Signaling pathway network inference}

\subsection{Constraint-based approaches}

\paragraph{ \citet{2018-koksal-tps}. Temporal Pathway Synthesizer (TPS). }




\subsection{Dynamic Bayesian Network (DBN) approaches}

\paragraph{ \citet{2017-hill-context}. Context specificity \ldots phosphoprotein profiling.}
The authors build on their previous works 
\citep{2015-spencer-interventional} 
\citep{2014-oates-joint}
to infer context-specific signaling pathways from phosphorylation data.
They consider (4 cell lines) $\times$ (8 perturbagens) $=$ 32 different contexts.
For each context they apply 6 different kinase inhibitor interventions, and incorporate the resulting causal information into their context-specific pathway reconstruction.
\citeauthor{2017-hill-context} validate the novel edges predicted by their method through western blot analysis.

\paragraph{ \citet{2015-spencer-interventional}. Inferring networks from interventional data.}

\paragraph{ \citet{2014-oates-joint}. Joint estimation of multiple related biological networks.}
The authors present a method for inferring multiple related networks simultaneously.
They do so by defining a hierarchical model whose leaves are networks;
they infer the network posteriors via belief propagation.
The results are exact, since the networks' parameters can be marginalized exactly and the hierarchical model contains no loops. 

\paragraph{ \citet{hill-bayesian-2012} Bayesian inference of signaling network topology.}
The authors present an exact Bayesian inference method for signaling pathways.
They use DBNs to represent the causal dependencies of phosphorylation events.
The strength of a DBN representation is that it leads to a straightforward Bayesian formulation:
$P(G|X) \propto \int P(X|G, \theta) \cdot P(G) \cdot P(\theta) d\theta$, where $P(X|G, \theta)$ is
simply the probability of the data given a particular DBN.
The authors also show that suitable priors $P(G)$ and $P(\theta)$ reduce the posterior $P(G|X)$
to a tractable sum (polynomial number of terms---the polynomial degree is given by an in-degree constraint on $G$).

\paragraph{ \citet{2010-robinson-nsdbn} Non-stationary DBNs.}
The authors describe a DBN variant which allows the network structure
to change at distinct ``breakpoints'' (my term, not theirs) in time.
They give model estimation methods for three settings: (1) known number of breakpoints/known times of breakpoints; (2) known number, unknown times; and (3) unknown number, unknown times.
The methods are MCMC-based---sampling takes place over the space of possible DBNs (and breakpoint parameters, when the setting calls for it).
The methods are able to identify the underlying model in a simulation involving 5000 observations of 100 random variables, with 4 breakpoints.
The authors then apply their methods to real-world datasets: gene regulation in fruit fly muscle development, and brain signals in birds.
Inference for a large problem instance entails compute times on the order of minutes for 100,000 samples.




\subsection{Belief Propagation/Prize-Collecting Steiner Tree approaches}

\paragraph{ \citet{2013-molinelli-perturbation}. Inferring networks for perturbed cells.}
The authors use the belief-propagation method described by \citeauthor{2012-biazzo-steiner} to infer 
MUST REREAD --- DO NOT YET FULLY GRASP THIS ONE.

\paragraph{ \citet{2012-biazzo-steiner}. Performance of a cavity-method-based algorithm for the prize-collecting Steiner tree problem on graphs.}
The authors explain how the PCST objective function can be regarded as a log-probability over trees, and then maximize it (approximately) using a belief-propagation algorithm.
They show that it compares favorably against other PCST solvers---typically based on integer program formulations---on a standard set of PCST benchmarks.
Their method usually converges faster \emph{and} attains higher scores than the others.

\paragraph{ \citet{2011-bailly-bechet-belief}. Finding undetected protein associations in cell signaling by belief propagation.}




\subsection{Other Works}

\paragraph{ \citet{2018-invergo-review}. Review of phosphorylation network reconstruction methods. }
This paper reviews a lot of the work in phosphorylation network reconstruction.
Of particular interest to us, though, was its description of the roles of kinases in phosphorylation networks. 
There are known relationships between kinases and proteins, with corresponding relationships in their phosphorylation events.
This kind of prior knowledge could be useful for network reconstruction.

\paragraph{ \citet{2016-hill-community}. Inferring causal molecular networks \ldots community-based effort.}
This paper describes the outcomes of a phosphorylation network reconstruction DREAM challenge.
Participants attempted to infer context-specific causal signaling networks from (a) simulated data and (b) real phosphorylation time-series measurements.
The participants submitted their networks, which were evaluated against held-out test sets.
The challenge evaluated a very particular notion of causality---i.e., two phosphorylation events ought to be causally related iff there exists a directed path from one to the other in the inferred network.
Two submissions reliably attained the highest scores; one was based on a notion of Granger causality and the other was based on a nonparametric hypothesis test for functional dependence of variables.
The latter did not even make use of prior biological knowledge (e.g., pathway databases curated by scientists).

\paragraph{ \citet{2008-mukherjee-priors}. Network inference using informative priors.}
The authors describe the network inference task and present a family of prior distributions for networks:
$ P(G) \propto \exp\left( \lambda \sum_i w_i f_i(G) \right).$ 
That is, the probability of a graph is some log-linear function of various graph features. 
\citeauthor{2008-mukherjee-priors} use this family of priors in conjunction with 
(a) exact marginal likelihood $P(X | G) = \int P(X | G, \theta) P(\theta) d\theta$, and
(b) MCMC sampling
to estimate expectations of features in the posterior network.
They evaluate their inference method via 
\begin{enumerate}
    \item a simulation study; they estimate the probabilities of edge existence in the posterior network
        and generate an ROC curve for these predictions.
    \item reconstructing a signaling pathway from phosphorylation data; they compare the most likely
        graph against expert knowledge of this particular pathway.
\end{enumerate}

\paragraph{ \citet{2010-vaske-paradigm}. Patient-specific pathway activities from multi-omic data.}

\bibliographystyle{plainnat}
\bibliography{biblio}

\pagebreak

\end{document}
